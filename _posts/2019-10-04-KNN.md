---
layout:     post
title: KNN
subtitle: machine learning
date:       2019-10-04
author:     Musen
header-img: img/post-bg-rwd.jpg
catalog: true
tags:
    - machine learning
---
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], processEscapes: true } }); </script> <script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

## KNN算法

---

### **1. KNN算法思想：**

- 我个人是这样理解KNN算法的，其实这是一种非常简单的分类方法，我们的，我们要知道测试样本的某种属性，可以通过该测试样本附近的已知样本来判断，这里有一个尺度来评判，就是距离，这个距离就是通过每个其他属性的数学参数来进行。里面很重要的一点就是k，k的取值要根据数据集的变化而变化。
  
- **网络资料解释：** 邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。Cover和Hart在1968年提出了最初的邻近算法。KNN是一种分类(classification)算法，它输入基于实例的学习（instance-based learning），属于懒惰学习（lazy learning）即KNN没有显式的学习过程，也就是说没有训练阶段，数据集事先已有了分类和特征值，待收到新样本后直接进行处理。与急切学习（eager learning）相对应。
  
- KNN是通过测量不同特征值之间的距离进行分类。 
思路是：如果一个样本在特征空间中的k个最邻近的样本中的大多数属于某一个类别，则该样本也划分为这个类别。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

---

###  **2. KNN算法的概述：**
- **A.计算测试数据与各个训练数据之间的距离；**
  
  我查了一下相关的资料，计算样本之间距离的公式有很多，数学中能够计算距离的公式都可以使用，比如欧几里得距离、余弦值（cos）, 相关度 （correlation）, 曼哈顿距离 （Manhattan distance），但是用得最多得还是欧式公式，具体公式如下：
 
- **B.选取合适的k值；**

  k值的选择与数据集的大小有关，通常情况下，数据集越大，k值越大，这样所得到结果的误差才会更小。

- **C.按照距离的递增关系进行排序；**
- **D.选取距离最小的K个点；**
- **E.确定前K个点所在类别的出现频率；**
- **F.返回前K个点中出现频率最高的类别作为测试数据的预测分类。**
  
---

### **3. KNN算法的伪代码**

![kNN.png](https://raw.githubusercontent.com/Musenming/musenming.github.io/master/img/kNN.png) 

---

### **4. Python代码实现**

```python
#KNN算法Python实现

import math

#计算距离，并将距离放入新的数组
def distance(dataframe,testdata):
    KNN = []
    for v in dataframe:
        d = math.sqrt((testdata[0] - v[0]) ** 2 + (testdata[1] - v[1]) ** 2)
        KNN.append([v[2],round(d, 4)])
    return KNN
#得到k个元素的距离新数组
def key(dataframe,testdata,k):
    KNN = distance(dataframe,testdata)
    KNN.sort(key=lambda x: x[1],reverse=True)
    KNN=KNN[:k]
    #print(KNN)
    return KNN
#预测样本的属性情况
def train(dataframe,testdata,k):
    KNN = key(dataframe,testdata,k)
    labels = [[1,0],[0,0]]#此数组用以统计属性频率
    for s in KNN:
        if s[0] == 1:
            labels[0][1] += 1
        else:
            labels[1][1] += 1
    labels.sort(key=lambda l: l[1],reverse=True)#按照频率大小进行排序
    if (labels[0][1]==labels[1][1]):
        labels[1][0]=labels[0][0]
        labels[0][0]=testdata[3]
    labels.insert(0,[testdata[2]]) 
    print("|",labels[0][0],"  |",labels[1][0],"  |",testdata[3]," |")
    return labels 
#训练集
movie_data = [[0.697,0.460,1],
              [0.634,0.264,1],
              [0.556,0.215,1],
              [0.481,0.149,1],
              [0.666,0.091,0],
              [0.245,0.057,0],
              [0.639,0.161,0],
              [0.360,0.370,0],
              [0.719,0.103,0]]
#测试集
test_data = [[0.774, 0.376,"2" ,1],
             [0.608, 0.318,"4" ,1],
             [0.403, 0.237,"6" ,1],
             [0.437, 0.211,"8" ,1],
             [0.243, 0.267,"10",0],
             [0.343, 0.099,"12",0],
             [0.657, 0.198,"14",0],
             [0.593, 0.042,"16",0]]

for k in range(1,6):
    correctcount = 0
    print("|编号","|预测","|实际|")
    for t in test_data:
        labels=train(movie_data,t,k)
        if labels[1][0] == t[3]:            
            correctcount +=1
    print(" k=",k,"  ","正确率：",correctcount/len(test_data),"\n")
```

---

### **5. 验证结果（含分类精度）：**

**k= 1    正确率： 0.5**

|编号 |预测 |实际|
|:---:|:---:|:---:|
| 2   | 0   | 1  |
| 4   | 0   | 1  |
| 6   | 1   | 1  |
| 8   | 1   | 1  |
| 10   | 0   | 0  |
| 12   | 1   | 0  |
| 14   | 0   | 0  |
| 16   | 1   | 0  |

**k= 2    正确率： 0.75** 

|编号 |预测 |实际|
|:---:|:---:|:---:|
| 2   | 0   | 1  |
| 4   | 0   | 1  |
| 6   | 1   | 1  |
| 8   | 1   | 1  |
| 10   | 0   | 0  |
| 12   | 0   | 0  |
| 14   | 0   | 0  |
| 16   | 0   | 0  |

**k= 3    正确率： 0.375**

|编号 |预测 |实际|
|:---:|:---:|:---:|
| 2   | 0   | 1  |
| 4   | 0   | 1  |
| 6   | 0   | 1  |
| 8   | 0   | 1  |
| 10   | 0   | 0  |
| 12   | 1   | 0  |
| 14   | 0   | 0  |
| 16   | 0   | 0  |


**k= 4    正确率： 0.5** 

|编号 |预测 |实际|
|:---:|:---:|:---:|
| 2   | 0   | 1  |
| 4   | 0   | 1  |
| 6   | 0   | 1  |
| 8   | 0   | 1  |
| 10   | 0   | 0  |
| 12   | 0   | 0  |
| 14   | 0   | 0  |
| 16   | 0   | 0  | 

**k= 5    正确率： 0.375**

|编号 |预测 |实际|
|:---:|:---:|:---:|
| 2   | 0   | 1  |
| 4   | 0   | 1  |
| 6   | 0   | 1  |
| 8   | 0   | 1  |
| 10   | 0   | 0  |
| 12   | 0   | 0  |
| 14   | 0   | 0  |
| 16   | 1   | 0  |

---

### **6. 总结**

**我跑出来也有点不敢相信，我有两点发现：**
- 1.正确率太低
- 2.正确率的变化并不是单纯地随着k值增高而增高，有一个下降的过程

> 结论：关于正确率太低的问题，我查了一些资料，一个原因是因为数据集太小，所以误差会很高网上的代码主要分为两类，一类是operator实现，一类是sklearn实现，我用sklearn实现的代码跑了一下确实提高了不少，因为用了数据包，不太好pdf上，就不贴出来.

---
