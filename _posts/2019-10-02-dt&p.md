---
layout:     post
title: Decision Tree & Pruning
subtitle: machine learning
date:       2019-10-02
author:     Musen
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - machine learning
---
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ['\\(','\\)'] ], processEscapes: true } }); </script> <script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

## 决策树练习（第四章课后）
#### 用决策树方法求解教材P76表4.1中数据（样本数据为表4.1中的奇数编号1.3.5.7.9.11.13.15.17；只有前两个条件属性变量色泽、根蒂）。

#### 1.	给出决策树及相应的布尔表达式：

**样本为：**

|编号|色泽|根蒂|   |
|:--:|:--:|:--:|:--:|
|1 |青绿 |蜷缩 |是|
|3 |乌黑 |蜷缩 |是|
|5 |浅白 |蜷缩 |是|
|7 |乌黑 |稍蜷 |是|
|9 |乌黑 |稍蜷 |否|
|11 |浅白 |硬挺 |否|
|13 |青绿 |稍蜷 |否|
|15 |乌黑 |稍蜷 |否|
|17 |青绿 |蜷缩 |否|

- 数据集含有两个训练样例, $| {\nu} |=2$
> ①开始学习时，根节点包含所有样例，正例占比为：$P_1=\frac{4}{9}$ ，反例比例为：$P_2=\frac{5}{9}$ 	
当前结点的信息熵为：

$$Ent(D)=-\sum_{k=1}^{2}P_k log_2P_k=0.9910760598382222$$
 
> ②当前属性合集为：{色泽，根蒂，敲声}，先选择色泽对D进行划分， $D_1$（色泽=青绿）,$D_2$（色泽=乌黑），$D_3$（色泽=浅白）,$D_1$的正例占比$\frac{1}{3}$ ，反例占比$\frac{2}{3}$；$D_2$的正例占比$\frac{1}{2}$，反例占比$\frac{1}{2}$，$D_3$的正例占比$\frac{1}{2}$，反例占比$\frac{1}{2}$，计算信息熵得到：
 
$$Ent(D_1)=-(\frac{1}{3}log_2\frac{1}{3}+\frac{2}{3}log_2\frac{2}{3})=0.9182958340544896$$

$$Ent(D_2)=-(\frac{1}{2}log_2\frac{1}{2}+\frac{1}{2}log_2\frac{1}{2})=1$$

$$Ent(D_2)=-(\frac{1}{2}log_2\frac{1}{2}+\frac{1}{2}log_2\frac{1}{2})=1$$
 
> ③信息增益为：

$$Gain(D,色泽)=Ent(D)-\sum_{\nu=1}^{3}\frac{|D^\nu|}{D}Ent(D^\nu)=0.018311$$
 
- 同样的计算方式：

$$Gain(D,根蒂)=Ent(D)-\sum_{\nu=1}^{3}\frac{|D^\nu|}{D}Ent(D^\nu)=0.269940$$
 
**所以根蒂被选择为划分属性,决策树如下：**

![jueceshu.jpg](https://raw.githubusercontent.com/Musenming/musenming.github.io/master/img/machine-learing-chapter4/jueceshu.jpg)
 
- 布尔表达式为：

$$ 好瓜=((根蒂=蜷缩)\land(色泽=乌黑))\lor((根蒂=蜷缩)\land(色泽=浅白))$$
 
- 计算信息增益和信息熵的代码如下：
  
```python
import math
def Ent(dataframe):
    numEntries=len(dataframe)
    labelCounts={}
    for featVec in dataframe:
        currentLabel=featVec[-1]
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel]=0
        labelCounts[currentLabel]+=1
    ent=0

    for key in labelCounts:
        prob=float(labelCounts[key])/numEntries 
        ent-=prob*math.log(prob,2)            
    return ent
def Gain(dataframe):
    numFeatures = len(dataframe[0]) - 1
    baseEntropy = Ent(dataframe)
    bestInfoGain = 0
    bestFeature = -1
    for i in range(numFeatures):
        featList = [example[i] for example in dataframe]
        uniqueVals = set(featList)
        newEntropy = 0
        for value in uniqueVals:
            subdataframe = splitdataframe(dataframe, i, value)
            prob = len(subdataframe) / float(len(dataframe))
            newEntropy += prob * Ent((subdataframe))
        infoGain = baseEntropy - newEntropy
        print("Gain(D%d)=%f" % (i, infoGain))
        if (infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature
def splitdataframe(dataframe,axis,value):
    retdataframe=[]
    for featVec in dataframe:
        if featVec[axis]==value:
            reducedFeatVec=featVec[:axis]
            reducedFeatVec.extend(featVec[axis+1:])
            retdataframe.append(reducedFeatVec)
    return retdataframe

dataframe=[
        [1,1,'yes'],[2,1,'yes'],[3,1,'yes'],
        [2,2,'yes'],[2,2,'no'],[3,3,'no'],
        [1,2,'no'],[2,2,'no'],[1,1,'no'],
]
labels=['色泽','根蒂']
print("Ent(D0)=",Ent(dataframe))
Gain(dataframe)
```

#### 2.	可以进一步考虑剪枝（以表4.1中的偶数编号样本为验证集）、或者增加条件属性变量（比如敲声、纹理等），以增强决策树的精度。

**验证集：**

|编号|色泽|根蒂|   |
|:--:|:--:|:--:|:--:|
|2 |乌黑 |蜷缩 |是|
|4 |青绿 |蜷缩 |是|
|6 |青绿 |稍蜷 |是|
|8 |乌黑 |稍蜷 |是|
|10 |青绿 |硬挺 |否|
|12 |浅白 |蜷缩 |否|
|14 |浅白 |稍蜷 |否|
|16 |浅白 |蜷缩 |否|
- #### **预剪枝：**
  
a) 该决策树的验证精度为：50%;

b)	划分根蒂属性：将三个结点标记为（好瓜、好瓜、坏瓜），验证精度为62.5%，所以用根蒂划分得以确定;

c)	然后再对1结点进行划分，会有结点导致判断错误，导致决策树验证精度下降，所以1结点不进行划分;

d)	对于结点2，同样会有样例被进行误判，但是会有样例会改变判断结果，判断正确，所以2结点可以划分；

e)	对于结点3，不用再划分

**所以预剪枝的决策树如下：**

![yujianzhi.jpg](https://raw.githubusercontent.com/Musenming/musenming.github.io/master/img/machine-learing-chapter4/yujianzhi.jpg)

- #### **后剪枝：**
  
a) 将1结点换为叶节点，该结点含有1、3、5、17四个样例，标记为好瓜，替换后样例4判断正确，验证精度提高，所以可以替换；

b) 将2结点换为叶节点，该结点含有7，9，13，15四个样例，标记为好瓜，替换后样例6、8判断正确，验证精度提高，所以可以替换。

**所以后剪枝的决策树如下：**

![houjianzhi.jpg](https://raw.githubusercontent.com/Musenming/musenming.github.io/master/img/machine-learing-chapter4/houjianzhi.jpg)
 



